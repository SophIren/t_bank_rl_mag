{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c4350af-9718-4738-9fc9-9ed88a0e242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "# %env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54c85b5a-27e5-4060-8db7-c74391f96a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from trl import RewardTrainer, RewardConfig\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef08fb4-b0d6-42c9-91e7-a309f68327fd",
   "metadata": {},
   "source": [
    "# Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2f846d5-a970-46f2-86f6-21efb83965fd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69293846-c470-46ad-9338-049918608c56",
   "metadata": {},
   "source": [
    "Загрузка модели и данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1a8e53-8dc0-4398-b159-f09c8398d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_name)\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(sft_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e17e3cc1-b9e6-4bbe-944e-c7771e9e4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"juyoungml/HelpSteer2-binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8eac9f8-a662-488a-bf85-97c4e00262ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc63abf-ec1d-4191-8147-1429db2e9f0f",
   "metadata": {},
   "source": [
    "Приведение датасета к implicit формату"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9b7d88a5-fbc2-458f-8e21-f40f4a49fb05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_conversational_format(example):\n",
    "    return {\n",
    "        \"chosen\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"chosen\"]},\n",
    "        ],\n",
    "        \"rejected\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"rejected\"]},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "train_data = train_data.map(to_conversational_format)\n",
    "val_data = val_data.map(to_conversational_format)\n",
    "\n",
    "train_data = train_data.remove_columns([\"prompt\", \"chosen_score\", \"rejected_score\", \"chosen_rationale\", \"rejected_rationale\", \"difficulty\", \"score_diff\"])\n",
    "val_data = val_data.remove_columns([\"prompt\", \"chosen_score\", \"rejected_score\", \"chosen_rationale\", \"rejected_rationale\", \"difficulty\", \"score_diff\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a3ff10-f90e-413c-a7b6-96569e0a60f2",
   "metadata": {},
   "source": [
    "Обучение reward модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42bb2a53-0f3d-49b4-b348-5b4ad81f45be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "rm_model = AutoModelForSequenceClassification.from_pretrained(sft_model_name, num_labels=1)\n",
    "training_args = RewardConfig(\n",
    "    output_dir=\"./reward_model_no_margin\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    max_length=4096 * 2,  # Сюда влезет почти все\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4db8984e-d2e7-4d05-af26-053268a5cf30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aed950f3a69f4b128d14f039535f88c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe187d93c3234afe8b4869df5d5ce3d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6b4964da9f3435f9561acca06f3092a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7224 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer = RewardTrainer(\n",
    "    model=rm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    processing_class=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1b7184d1-6157-4a78-9665-5dc6dd14fe3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='903' max='903' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [903/903 19:51, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.750700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.703400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.664400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.724800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.661200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.675600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.627800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.612800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.604600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.636900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.667800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.662400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.623500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.630100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.708700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.625600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.673200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.678100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.626000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.567900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.603700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.647900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.588200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.648100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.631900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.617000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.645900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.566500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.595700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.618700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.550400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.622900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.613400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.614400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.567500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.575200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.533300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.595100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.561900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.665200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.573400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=903, training_loss=0.6300229396534917, metrics={'train_runtime': 1192.7671, 'train_samples_per_second': 6.057, 'train_steps_per_second': 0.757, 'total_flos': 0.0, 'train_loss': 0.6300229396534917, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8768fdb1-07c4-442c-96da-188abb534a59",
   "metadata": {},
   "source": [
    "Измерение количества пар $r_w > r_l$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9fd3a439-6d1e-485f-871e-720f61498b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_model = AutoModelForSequenceClassification.from_pretrained('./reward_model_no_margin/checkpoint-903', num_labels=1).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f9175a4-5e9b-410f-bc49-614b6d97a040",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8560834be68f4fa690497e8e3aaaeed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rm_model.eval()\n",
    "\n",
    "correct = 0\n",
    "for sample in tqdm(val_data):\n",
    "    chosen = tokenizer.apply_chat_template(sample[\"chosen\"], tokenize=False)\n",
    "    rejected = tokenizer.apply_chat_template(sample[\"rejected\"], tokenize=False)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        [chosen, rejected],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096 * 2\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = rm_model(**inputs)\n",
    "        scores = outputs.logits.squeeze().tolist()\n",
    "\n",
    "    if scores[0] > scores[1]:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb1dc474-4afe-4c22-b77f-aea0c911ef74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6648793565683646"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb50368-cb25-4f4f-ac1a-330ee61eb33f",
   "metadata": {},
   "source": [
    "Результат обучения на метрике так себе (но все еще лучше, чем 0.5). Я думаю, что нужно обучать дольше и/или с другими гиперпараметрами, но они зафиксированы в задании, поэтому дальше работа будет проводится с этой моделью."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914f3202-ec38-46a1-afe8-7d61503f7f4c",
   "metadata": {},
   "source": [
    "Так как в датасете доступны скоры ответов, можно использовать и их для обучения\n",
    "https://huggingface.co/docs/trl/main/en/reward_trainer#adding-a-margin-to-the-loss\n",
    "\n",
    "В результате получилась reward модель немного хуже по метрике количества $r_w > r_l$ из валидационной выборки (код см ниже)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12966f59-0944-4c41-b172-4dbfb670b287",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65536d1-0490-48e3-9f56-6627f2063028",
   "metadata": {},
   "source": [
    "Загрузка данных и модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e88826b-2260-4b7d-8a1a-da6c5c61c2b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=576, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(sft_model_name).cuda()\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained('reward_model_no_margin/checkpoint-903').cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_name)\n",
    "\n",
    "policy_model.train()\n",
    "rm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f84ff64f-7541-4292-a6e8-ba0f6b82e570",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512  # Здесь уже меньше, чем в reward модели для экономии памяти.\n",
    "# Однако в reward модель будем подавать необрезанные prompt и сгенерированный ответ\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding_side='left',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4fc4f23-820b-4f31-a448-cd113b6730b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"juyoungml/HelpSteer2-binarized\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"juyoungml/HelpSteer2-binarized\", split=\"validation\")\n",
    "\n",
    "dataset = dataset.map(preprocess_function)\n",
    "eval_dataset = eval_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01296c51-bdc7-44f6-aa2c-080d7b6ce0b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 6\n",
    "max_gen_len = 128\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"prompt\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"prompt\"])\n",
    "loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30050496-1179-48f0-9f69-7163f26ed30f",
   "metadata": {},
   "source": [
    "Скользящее среднее в качестве бейзлайна"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e509cb54-b010-44e1-b0f5-c9390ca6d9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAverage:\n",
    "    def __init__(self):\n",
    "        self._total_reward = 0\n",
    "        self._count = 0\n",
    "\n",
    "    @property\n",
    "    def average(self):\n",
    "        return self._total_reward / self._count\n",
    "\n",
    "    def __call__(self, rewards):\n",
    "        for i in range(len(rewards)):  # Итерируемся по каждому реворду из батча по отдельности\n",
    "            self._total_reward += rewards[i]\n",
    "            self._count += 1\n",
    "            rewards[i] -= self.average\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "957b4955-a1ad-4f2c-aa23-72fdd011b9c8",
   "metadata": {},
   "source": [
    "Функция подсчета награды на валидационной выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd296428-0f4a-4b4a-8b40-f1f2d953966c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_reward(policy_model):\n",
    "    with torch.no_grad():\n",
    "        total_reward = 0\n",
    "        for batch in tqdm(eval_loader):\n",
    "            input_ids = batch[\"input_ids\"].to('cuda')\n",
    "            attention_mask = batch[\"attention_mask\"].to('cuda')\n",
    "            outputs = policy_model.generate(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    max_length=input_ids.size(1) + max_gen_len,\n",
    "                    do_sample=True,\n",
    "                    top_k=50,\n",
    "                    return_dict_in_generate=True,\n",
    "                )\n",
    "            \n",
    "            sequences = outputs.sequences\n",
    "            gen_sequences = sequences[:, input_ids.size(1):]\n",
    "        \n",
    "            gen_texts = tokenizer.batch_decode(gen_sequences, skip_special_tokens=True)\n",
    "            texts = []\n",
    "            for i in range(len(batch['prompt'])):\n",
    "                texts.append([\n",
    "                    {\"role\": \"user\", \"content\": batch['prompt'][i]},\n",
    "                    {\"role\": \"assistant\", \"content\": gen_texts[i]}\n",
    "                ])\n",
    "            chat = tokenizer.apply_chat_template(texts, tokenize=False)  # Приведение в conversational формат, на котором обучалась rm\n",
    "            enc_rm = tokenizer(\n",
    "                chat,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=4096,  # Здесь можно использовать длину больше, чем для промпта sft модели\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            total_reward += rm_model(**enc_rm).logits.squeeze(-1).mean().item()\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a98a32ac-c806-426d-b228-f0456d3843be",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_gen_len = 128\n",
    "n_iter = 0\n",
    "cum_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10514521-0fd0-4d60-a9ee-1cd0c12e5b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10be9a25c0f740ce863b628d3bf3279c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5bcb2779090489bb40aa872cf1a3570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "eval_reward: -287.66557264328003\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(policy_model.parameters(), lr=1e-5)  # lr=1e-5 зафиксирован для любых дообучений sft модели\n",
    "rewards_history = []\n",
    "mov_avg = MovingAverage()\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in tqdm(loader):\n",
    "        input_ids = batch[\"input_ids\"].to('cuda')\n",
    "        attention_mask = batch[\"attention_mask\"].to('cuda')\n",
    "        outputs = policy_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=input_ids.size(1) + max_gen_len,\n",
    "                do_sample=True,  # Здесь присутствует некоторая случайность, но это должно способствовать обучению лучше, чем argmax\n",
    "                top_k=50,\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "        \n",
    "        sequences = outputs.sequences\n",
    "        gen_sequences = sequences[:, input_ids.size(1):]\n",
    "\n",
    "        # Подсчет log(pi(y|x))\n",
    "        gen_logits = policy_model(sequences).logits[:, input_ids.size(1) - 1:-1, :]  # Логиты только сгенерированных токенов\n",
    "        gen_log_probs = F.log_softmax(gen_logits, dim=-1)\n",
    "        gen_log_probs = torch.gather(gen_log_probs, 2, gen_sequences.unsqueeze(-1)).squeeze(-1)  # Лог оценок вероятностей сгенерированных токенов\n",
    "        gen_pad_mask = (gen_sequences != tokenizer.pad_token_id).float()\n",
    "        # log(p(X)) = log(p(x_1) * p(x_2|x_1) * p(x_3|x_1, x_2) * ...) = log(p(x_1)) + log(p(x_2)) + ...\n",
    "        gen_log_probs = (gen_log_probs * gen_pad_mask).sum(dim=1)  # Вроятность pad токенов не входит в оценку вероятности сгенерированного текста\n",
    "    \n",
    "        gen_texts = tokenizer.batch_decode(gen_sequences, skip_special_tokens=True)\n",
    "        texts = []\n",
    "        for i in range(len(batch['prompt'])):\n",
    "            texts.append([\n",
    "                {\"role\": \"user\", \"content\": batch['prompt'][i]},\n",
    "                {\"role\": \"assistant\", \"content\": gen_texts[i]}\n",
    "            ])\n",
    "        chat = tokenizer.apply_chat_template(texts, tokenize=False)  # Приведение в conversational формат, на котором обучалась rm\n",
    "        enc_rm = tokenizer(\n",
    "            chat,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=4096,  # Здесь можно использовать длину больше, чем для промпта sft модели\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            rewards = rm_model(**enc_rm).logits.squeeze(-1)\n",
    "    \n",
    "        loss = -(mov_avg(rewards) * gen_log_probs).mean()  # loss из статьи\n",
    "        \n",
    "        if n_iter % 20 == 0 and n_iter > 0:\n",
    "            print(f'n_iter: {n_iter}, loss: {cum_loss / 20}')\n",
    "            cum_loss = 0\n",
    "        if n_iter % 200 == 0:\n",
    "            policy_model.eval()\n",
    "            r = eval_reward(policy_model)  # Каждые 200 итераций смотрим на reward валидационной выборки\n",
    "            rewards_history.append(r)\n",
    "            print(f'\\neval_reward: {r}')\n",
    "            print()\n",
    "            policy_model.train()\n",
    "\n",
    "        cum_loss += loss.item()\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        n_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f305d6a4-85b1-4a36-a339-9a0f34c479fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-287.66557264328003,\n",
       " -272.61764550209045,\n",
       " -253.94394040107727,\n",
       " -240.6589252948761,\n",
       " -226.70869159698486,\n",
       " -226.89586567878723,\n",
       " -220.69964003562927,\n",
       " -206.26513159275055,\n",
       " -216.38121724128723,\n",
       " -208.6601858139038,\n",
       " -200.4552869796753,\n",
       " -200.17305254936218,\n",
       " -204.647110581398,\n",
       " -195.3173370361328,\n",
       " -190.11309719085693,\n",
       " -196.9596209526062,\n",
       " -199.98877155780792]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_history  # История ревордов каждые 200 итераций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29e41b8-0ab4-42ed-9e8f-85b14094903a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3233"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_iter  # Всего итераций для bs=6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ffcc11f-9f77-40e9-b3fe-446c4ba91229",
   "metadata": {},
   "source": [
    "Финальный reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "475b9212-447a-41b0-8922-b63a1b2b594d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b344929e17504cad84d16be62f8e362d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-193.72901272773743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_model.eval()\n",
    "r = eval_reward(policy_model)\n",
    "print(r)\n",
    "policy_model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d0b0e01-310b-46ff-aebb-445298437fa0",
   "metadata": {},
   "source": [
    "Реворд не дообученной sft модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200e11c2-eadb-4ea0-9f4f-393c22bc4231",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85679f855b7e4342b57175b43bfa08b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-290.31163215637207\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=576, out_features=49152, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_model = AutoModelForCausalLM.from_pretrained(sft_model_name).cuda()\n",
    "original_model.eval()\n",
    "print(eval_reward(original_model))\n",
    "original_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5877fe08-ffcf-4bf2-9fd4-445ebe79c387",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model.save_pretrained('./policy_model_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651d779c-ddaf-4d66-af91-e43880d7e432",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Доп: reward model с margin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a545cb19-5089-4cd4-95bd-90929686c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_name)\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(sft_model_name, num_labels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f9b014-0dc3-4e3e-83b3-8d3318b22fa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"juyoungml/HelpSteer2-binarized\")\n",
    "train_data = dataset['train']\n",
    "val_data = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1a4fe0-9c12-4a94-b069-7cc6646112b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_conversational_format(example):\n",
    "    return {\n",
    "        \"chosen\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"chosen\"]},\n",
    "        ],\n",
    "        \"rejected\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"rejected\"]},\n",
    "        ],\n",
    "    }\n",
    "\n",
    "train_data = train_data.map(to_conversational_format)\n",
    "val_data = val_data.map(to_conversational_format)\n",
    "\n",
    "train_data = train_data.remove_columns([\"prompt\", \"chosen_score\", \"rejected_score\", \"chosen_rationale\", \"rejected_rationale\", \"difficulty\"])\n",
    "val_data = val_data.remove_columns([\"prompt\", \"chosen_score\", \"rejected_score\", \"chosen_rationale\", \"rejected_rationale\", \"difficulty\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c1e5a4-15ae-42a1-a0db-65d2b30ffbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.rename_column(\"score_diff\", \"margin\")  # Чтобы trl сам включил в лосс\n",
    "val_data = val_data.rename_column(\"score_diff\", \"margin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18ba33-500f-4852-9747-7076aaaff389",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = RewardConfig(\n",
    "    output_dir=\"./reward_model\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    gradient_checkpointing=True,  # reduce memory usage but train ~30% slower\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    max_length=4096 * 2,  # Сюда влезет почти все\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2132c983-e5d2-4084-b24a-82be0e49ec3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardTrainer(\n",
    "    model=rm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8d48e4-a56e-4c4b-ac6c-06c34c7a51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "rm_model.eval()\n",
    "\n",
    "correct = 0\n",
    "for sample in tqdm(val_data):\n",
    "    chosen = tokenizer.apply_chat_template(sample[\"chosen\"], tokenize=False)\n",
    "    rejected = tokenizer.apply_chat_template(sample[\"rejected\"], tokenize=False)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        [chosen, rejected],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096 * 2\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = rm_model(**inputs)\n",
    "        scores = outputs.logits.squeeze().tolist()\n",
    "\n",
    "    if scores[0] > scores[1]:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d71cfa31-9b87-43df-8b64-a652d6d4f481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6407506702412868\n"
     ]
    }
   ],
   "source": [
    "correct / len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ca9e64-8440-4fbb-be0c-89355e99bd1f",
   "metadata": {},
   "source": [
    "## Отчет Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f963f51-d3ce-4027-b8c8-8d97a8ba5edc",
   "metadata": {},
   "source": [
    "- Обучена reward модель с посредственной метрикой (0.66) количества доли пар угаданных оценок ответов $r_w > r_l$ с заданными гиперпараметрами и 1 эпохой. Предположительно, результат не очень из-за недообучения\n",
    "- Добавление margin в loss reward модели не дал значимых изменений в метрике. Думаю, что также из-за недообучения\n",
    "- С использованием reward модели реализован алгоритм reinforce с moving average baseline, который дал значительный прирост в средней награде на дообученной на нем sft моделе\n",
    "- Итак, удалось добится reward=-193 на валидационной выборке после 3 эпох обучения sft модели по сравнению с не дообученной sft моделью с reward=-290. Средняя награда на отложенной выборке определенно значительно выросла. Это закономерный результат\n",
    "- Стоит учесть, что, несмотря на то, что награда значительно выросла, реальное качество ответов может не улучшится или даже ухудшится из-за посредственной reward модели, как было замечено раннее"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43c0236-a3c5-466b-a528-321934d557a9",
   "metadata": {},
   "source": [
    "# Level 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b1ed00-28b2-4eaa-aae1-1c748ef28dad",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reward model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69b78c-06f7-4ccf-b678-3dbc032bb96a",
   "metadata": {},
   "source": [
    "Загрузка моделей и данных, аналогично level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa2d7b52-a436-4d62-b38a-9bbb6ebe4867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at HuggingFaceTB/SmolLM2-135M-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "sft_model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_name)\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    sft_model_name,\n",
    "    num_labels=10,  # Модель возвращает логиты для 10 классов (оценок)\n",
    "    problem_type=\"single_label_classification\",  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d67eabd-4cb3-4237-b218-c9b9083e3efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"juyoungml/HelpSteer2-binarized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3eb40e1a-6ec3-43b0-a0e3-e848e0f4fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_conversational_format(example):\n",
    "    return {\n",
    "        \"chosen\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"chosen\"]},\n",
    "        ],\n",
    "        \"rejected\":[\n",
    "            {\"role\": \"user\", \"content\": example[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"rejected\"]},\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7dee369-fa0e-491a-9752-c626c8b0fed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dataset[\"train\"].map(to_conversational_format).remove_columns(\n",
    "    [\"prompt\",\"chosen_score\",\"rejected_score\",\"chosen_rationale\",\"rejected_rationale\",\"difficulty\",\"score_diff\"]\n",
    ")\n",
    "val_data = dataset[\"validation\"].map(to_conversational_format).remove_columns(\n",
    "    [\"prompt\",\"chosen_score\",\"rejected_score\",\"chosen_rationale\",\"rejected_rationale\",\"difficulty\",\"score_diff\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18df7d6b-3ae0-4b4d-8b13-05414bd9e8d6",
   "metadata": {},
   "source": [
    "В обычной rlhf используется $loss = -log(\\sigma(r_w - r_l))$\n",
    "\n",
    "Мы хотим максимизировать вероятность $p(y_w \\succ y_l | x)$, имея $p_w(i)$ и $p_l(i)$, $i = 1...10$ - вероятность получить оценку i\n",
    "\n",
    "Для этого можно применить $loss = -log \\sum_{i=2}^{10}\\sum_{j=1}^{i-1}p_w(i)*p_l(j)$ - просто явно посчитали $p(y_w > y_l)$ и навесили log, как в оригинальном лоссе\n",
    "\n",
    "У этого лосса есть проблема, что он не учитывает, насколько далеки между собой i и j. То есть $p_w = (1, 0, ..., 0)$, $p_l = (0, ..., 0, 1)$ будет так же плохо, как $p_w = (1, 0, ..., 0)$, $p_l = (0, 1, 0 ..., 0)$\n",
    "\n",
    "Чтобы учитывать это, можно применить что-то вроде аналога margin. Домножить каждое слагаемое на $\\sigma(i - j)$.\n",
    "\n",
    "Реализуем обучение reward модели с $loss = -log \\sum_{i=1}^{10}\\sum_{j=1}^{10}p_w(i)*p_l(j)*\\sigma(i - j)$.\n",
    "\n",
    "Также это должно сгладить функцию ошибки, что по идее должно способствовать обучению."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142667eb-b9da-44ee-a6d5-1693b01928ab",
   "metadata": {},
   "source": [
    "Можно также добавить гиперпараметр $\\alpha$\n",
    "\n",
    "$loss = -log \\sum_{i=1}^{10}\\sum_{j=1}^{10}p_w(i)*p_l(j)*\\sigma(\\alpha * (i - j))$\n",
    "\n",
    "Для эксперимента возьму небольшое alpha, однако это уже позволит избежать случая, описанного выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22c460be-f0a9-4815-89ce-9e0b87af1ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.2\n",
    "\n",
    "class CustomRewardTrainer(RewardTrainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        if return_outputs or num_items_in_batch is not None:\n",
    "            raise RuntimeError('return_outputs=True or num_items_in_batch not None')\n",
    "\n",
    "        # получаем logits\n",
    "        logits_w = model(input_ids=inputs['input_ids_chosen'], attention_mask=inputs['attention_mask_chosen']).logits\n",
    "        logits_l = model(input_ids=inputs['input_ids_rejected'], attention_mask=inputs['attention_mask_rejected']).logits\n",
    "\n",
    "        # Оценки вероятностей\n",
    "        p_w = F.softmax(logits_w, dim=-1)\n",
    "        p_l = F.softmax(logits_l, dim=-1)\n",
    "\n",
    "        # Строим матрицу σ(i-j)\n",
    "        device = logits_w.device\n",
    "        idx = torch.arange(p_w.size(-1), device=device)\n",
    "        i, j = idx[:, None], idx[None, :]\n",
    "        sigma = torch.sigmoid(ALPHA * (i - j)).to(device)\n",
    "\n",
    "        # Подсчет loss\n",
    "        pref = (p_w.unsqueeze(2) * p_l.unsqueeze(1) * sigma).sum(dim=(1, 2))\n",
    "        loss = -torch.log(pref + 1e-8).mean()  # Добавление 1e-8, чтобы избежать здесь ошибки\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8ab355-1e4b-47a2-bf2c-5fb701a97e4c",
   "metadata": {},
   "source": [
    "Те же гиперпараметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae17c80f-df41-487e-8b63-ae458d7ed01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = RewardConfig(\n",
    "    output_dir=\"./reward_model_with_dist\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    logging_steps=20,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    gradient_accumulation_steps=1,\n",
    "    learning_rate=5e-5,\n",
    "    fp16=True,\n",
    "    max_length=4096 * 2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "566efc30-3cb3-4a8c-9d00-7114b44e037f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='903' max='903' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [903/903 19:47, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.692200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.694500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.687000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.664200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.683600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.690400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.693000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.693200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.692800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.693600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>420</td>\n",
       "      <td>0.688800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>440</td>\n",
       "      <td>0.672400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>460</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>480</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>520</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.692900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>560</td>\n",
       "      <td>0.695100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>580</td>\n",
       "      <td>0.690500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.696800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>620</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.679400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>660</td>\n",
       "      <td>0.677700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>680</td>\n",
       "      <td>0.702000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.686600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>720</td>\n",
       "      <td>0.677300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>740</td>\n",
       "      <td>0.678300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>760</td>\n",
       "      <td>0.644900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>780</td>\n",
       "      <td>0.663900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>820</td>\n",
       "      <td>0.641800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>840</td>\n",
       "      <td>0.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>860</td>\n",
       "      <td>0.673400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>880</td>\n",
       "      <td>0.667100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.647000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=903, training_loss=0.6836542161728193, metrics={'train_runtime': 1188.8209, 'train_samples_per_second': 6.077, 'train_steps_per_second': 0.76, 'total_flos': 0.0, 'train_loss': 0.6836542161728193, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = CustomRewardTrainer(\n",
    "    model=rm_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=val_data,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94fa83c-f93a-4b52-ae3f-57f66db02113",
   "metadata": {},
   "source": [
    "Посмотрим на ту же метрику качества, что и в level 1. Для это придется посчитать мат ожидание предсказанных оценок и сравнить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d180ae23-5b62-4a88-9dca-298322df959c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8e890737f914ef4b7fd68893be00a65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/373 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rm_model.eval()\n",
    "\n",
    "correct = 0\n",
    "for sample in tqdm(val_data):\n",
    "    chosen = tokenizer.apply_chat_template(sample[\"chosen\"], tokenize=False)\n",
    "    rejected = tokenizer.apply_chat_template(sample[\"rejected\"], tokenize=False)\n",
    "    \n",
    "    inputs = tokenizer(\n",
    "        [chosen, rejected],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=4096 * 2\n",
    "    ).to(\"cuda\")\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = rm_model(**inputs)\n",
    "        scores = outputs.logits.squeeze()\n",
    "        p = F.softmax(scores, dim=-1)\n",
    "        ratings = torch.arange(1, 11, device=p.device).float()\n",
    "        exp_r = (p * ratings).sum(dim=1)\n",
    "\n",
    "    if exp_r[0] > exp_r[1]:\n",
    "        correct += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e524f83f-10ba-47b4-99ba-9ecfad3d4378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.613941018766756"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct / len(val_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2547c2e9-0ac5-4695-9dd5-5c6aa19bfb0c",
   "metadata": {},
   "source": [
    "Качество по этой метрике получилось совсем немного хуже level 1, но зато теперь мы имеем целое распределение над оценками, а не просто скаляр"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612caa74-9d46-44fb-b092-086144188c2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Reinforce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3dd8913-55a3-478b-b211-3335d7bb4fc5",
   "metadata": {},
   "source": [
    "Аналогично level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7e36be-b59a-4c53-8bce-24507101b015",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=576, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "policy_model = AutoModelForCausalLM.from_pretrained(sft_model_name).cuda()\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained('reward_model_with_dist/checkpoint-903').cuda()\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_name)\n",
    "\n",
    "policy_model.train()\n",
    "rm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "376824f5-f841-4374-8ac1-11fa906271cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH = 512\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"prompt\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=MAX_LENGTH,\n",
    "        padding_side='left',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33828d91-2554-4067-aa85-7dbec9ea1070",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"juyoungml/HelpSteer2-binarized\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"juyoungml/HelpSteer2-binarized\", split=\"validation\")\n",
    "\n",
    "dataset = dataset.map(preprocess_function)\n",
    "eval_dataset = eval_dataset.map(preprocess_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85627bc4-eedc-452c-b2b2-21a7bc282411",
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 6\n",
    "max_gen_len = 128\n",
    "dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"prompt\"])\n",
    "eval_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"prompt\"])\n",
    "loader = DataLoader(dataset, batch_size=bs, shuffle=True)\n",
    "eval_loader = DataLoader(eval_dataset, batch_size=bs, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61794039-e635-48a4-b9ef-6893295af88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovingAverage:\n",
    "    def __init__(self):\n",
    "        self._total_reward = 0\n",
    "        self._count = 0\n",
    "\n",
    "    @property\n",
    "    def average(self):\n",
    "        return self._total_reward / self._count\n",
    "\n",
    "    def __call__(self, rewards):\n",
    "        for i in range(len(rewards)):\n",
    "            self._total_reward += rewards[i]\n",
    "            self._count += 1\n",
    "            rewards[i] -= self.average\n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e74bf1bb-98fc-416f-a265-51f6df62c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "mov_avg = MovingAverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5010ffbe-0c58-4269-96fa-51dabf89dd00",
   "metadata": {},
   "source": [
    "Так как теперь у нас есть информация обо всем распределении, можно интегрировать ее в алгоритм reinforce, используя дополнительную информацию.\n",
    "\n",
    "Понятно, что можно посчитать мат ожидание по распределению оценок и использовать его вместо скаляра из level 1\n",
    "\n",
    "Помимо этого, также будем немного поощрять модель за сохранение энтропии в распределении. Это будет останавливать ее скатитываться в окрас ответов на хорошие и плохие, тем более что reward модель у нас показывает не очень хорошее качество. Ответы будут более разнообразными. Мы не могли бы применить такое, не имея распределения над оценками в rm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45557d70-5cc4-4e0f-8e0d-4c725e6d343a",
   "metadata": {},
   "source": [
    "Также добавим еще регуляризационный коэффициент для энтропии = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c0fb93-5489-4bfb-83c6-c96d02b72a90",
   "metadata": {},
   "source": [
    "В остальном, все аналогично level 1, но eval_reward здесь уже возвращает среднюю награду (просто нюанс)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f953ce8a-2e60-4e23-ad60-183cfe62515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_reward_dist(policy_model):\n",
    "    policy_model.eval()\n",
    "    total_reward = 0.0\n",
    "    n = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(eval_loader, desc=\"Eval\"):\n",
    "            input_ids = batch[\"input_ids\"].cuda()\n",
    "            attention_mask = batch[\"attention_mask\"].cuda()\n",
    "            outputs = policy_model.generate(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                max_length=input_ids.size(1) + max_gen_len,\n",
    "                do_sample=True,\n",
    "                top_k=50,\n",
    "                return_dict_in_generate=True,\n",
    "            )\n",
    "            gen_sequences = outputs.sequences[:, input_ids.size(1):]\n",
    "            gen_texts = tokenizer.batch_decode(gen_sequences, skip_special_tokens=True)\n",
    "            chats = [\n",
    "                [\n",
    "                    {\"role\": \"user\", \"content\": batch['prompt'][i]},\n",
    "                    {\"role\": \"assistant\", \"content\": gen_texts[i]}\n",
    "                ]\n",
    "                for i in range(len(gen_texts))\n",
    "            ]\n",
    "            chat_str = tokenizer.apply_chat_template(chats, tokenize=False)\n",
    "            enc = tokenizer(\n",
    "                chat_str,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation=True,\n",
    "                padding=True,\n",
    "                max_length=4096,\n",
    "            ).to(\"cuda\")\n",
    "            \n",
    "            logits = rm_model(**enc).logits\n",
    "            p = F.softmax(logits, dim=-1)\n",
    "            ratings = torch.arange(1, 11, device=p.device).float()\n",
    "            exp_r = (p * ratings).sum(dim=1)  # Мат ожидание распределения оценок\n",
    "            # энтропия RM: H(p) = -sum p log p\n",
    "            ent = (-p * torch.log(p + 1e-8)).sum(dim=1)\n",
    "            total_reward += ((exp_r - entropy_coeff * ent).sum().item())  # Общий reward с entropy_coeff\n",
    "            n += exp_r.size(0)\n",
    "    policy_model.train()\n",
    "    return total_reward / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57426bfe-7873-496e-8eb8-a7d66275ba2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 0\n",
    "cum_loss = 0.0\n",
    "rewards_history = []\n",
    "entropy_coeff = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7261bce-c0e3-450d-9c57-326a4f3068ee",
   "metadata": {},
   "source": [
    "Столько же итераций, как в level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a3be0e07-d708-417e-bdca-4c3ad73a47b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c530373557e74e5699883b19913bf2e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/1204 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 00020  avg_loss=10.2487\n",
      "Iter 00040  avg_loss=72.9807\n",
      "Iter 00060  avg_loss=16.6656\n",
      "Iter 00080  avg_loss=-12.3954\n",
      "Iter 00100  avg_loss=107.6640\n",
      "Iter 00120  avg_loss=30.4735\n",
      "Iter 00140  avg_loss=106.6436\n",
      "Iter 00160  avg_loss=114.3030\n",
      "Iter 00200  avg_loss=101.7930\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7ad5bef22a94e6e99a5abc0aadbc2cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> eval_reward: 3.4773\n",
      "\n",
      "Iter 00220  avg_loss=146.9995\n",
      "Iter 00240  avg_loss=116.0670\n",
      "Iter 00260  avg_loss=30.8886\n",
      "Iter 00280  avg_loss=113.5096\n",
      "Iter 00300  avg_loss=71.1413\n",
      "Iter 00320  avg_loss=13.4653\n",
      "Iter 00340  avg_loss=4.4304\n",
      "Iter 00360  avg_loss=-62.0211\n",
      "Iter 00380  avg_loss=9.9224\n",
      "Iter 00400  avg_loss=-471.0392\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b57815fc9b05449287b78fd79e9ecb9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> eval_reward: 3.7778\n",
      "\n",
      "Iter 00420  avg_loss=-97.4498\n",
      "Iter 00440  avg_loss=-178.8072\n",
      "Iter 00460  avg_loss=-685.1604\n",
      "Iter 00480  avg_loss=373.7583\n",
      "Iter 00500  avg_loss=-105.0911\n",
      "Iter 00520  avg_loss=-22.7595\n",
      "Iter 00540  avg_loss=-706.0411\n",
      "Iter 00560  avg_loss=-18.2473\n",
      "Iter 00580  avg_loss=223.8146\n",
      "Iter 00600  avg_loss=-202.1060\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b984e23ae13741da9a8d17d4e643a9ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> eval_reward: 4.1183\n",
      "\n",
      "Iter 00620  avg_loss=-870.7336\n",
      "Iter 00640  avg_loss=-314.8506\n",
      "Iter 00660  avg_loss=-526.4296\n",
      "Iter 00680  avg_loss=-56.1356\n",
      "Iter 00700  avg_loss=122.9168\n",
      "Iter 00720  avg_loss=172.6957\n",
      "Iter 00740  avg_loss=-485.0805\n",
      "Iter 00760  avg_loss=-581.7384\n",
      "Iter 00780  avg_loss=-985.2548\n",
      "Iter 00800  avg_loss=-533.3695\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01fdfd92e4d4dbcba4f357eaf97b7a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> eval_reward: 4.2147\n",
      "\n",
      "Iter 00820  avg_loss=-1777.8043\n",
      "Iter 00840  avg_loss=-1761.4774\n",
      "Iter 00860  avg_loss=-750.6438\n",
      "Iter 00880  avg_loss=-1979.2062\n",
      "Iter 00900  avg_loss=-1984.1066\n",
      "Iter 00920  avg_loss=195.9078\n",
      "Iter 00940  avg_loss=-1521.1585\n",
      "Iter 00960  avg_loss=-1936.7599\n",
      "Iter 00980  avg_loss=-1071.8721\n",
      "Iter 01000  avg_loss=-1556.7316\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3424f15bf4ad42428826dd0f97aaf6c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">> eval_reward: 4.2917\n",
      "\n",
      "Iter 01020  avg_loss=-1038.5451\n",
      "Iter 01040  avg_loss=-1183.2266\n",
      "Iter 01060  avg_loss=-1124.0564\n",
      "Iter 01080  avg_loss=-2015.5051\n",
      "Iter 01100  avg_loss=-1471.6347\n",
      "Iter 01120  avg_loss=-1390.9599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(policy_model.parameters(), lr=1e-5)\n",
    "\n",
    "for epoch in range(3):\n",
    "    for batch in tqdm(loader, desc=f\"Epoch {epoch+1}\"):\n",
    "        input_ids = batch[\"input_ids\"].cuda()\n",
    "        attention_mask = batch[\"attention_mask\"].cuda()\n",
    "\n",
    "        # Генерация\n",
    "        outputs = policy_model.generate(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            max_length=input_ids.size(1) + max_gen_len,\n",
    "            do_sample=True,\n",
    "            top_k=50,\n",
    "            return_dict_in_generate=True,\n",
    "        )\n",
    "        sequences = outputs.sequences\n",
    "        gen_sequences = sequences[:, input_ids.size(1):]\n",
    "\n",
    "        # Лог-пробы π\n",
    "        gen_logits = policy_model(sequences).logits[:, input_ids.size(1)-1:-1, :]\n",
    "        log_probs = F.log_softmax(gen_logits, dim=-1)\n",
    "        log_probs = torch.gather(log_probs, 2, gen_sequences.unsqueeze(-1)).squeeze(-1)\n",
    "        mask = (gen_sequences != tokenizer.pad_token_id).float()\n",
    "        seq_logprob = (log_probs * mask).sum(dim=1)\n",
    "\n",
    "        # RM ожидаемая оценка + энтропия\n",
    "        gen_texts = tokenizer.batch_decode(gen_sequences, skip_special_tokens=True)\n",
    "        chats = [\n",
    "            [\n",
    "                {\"role\": \"user\", \"content\": batch['prompt'][i]},\n",
    "                {\"role\": \"assistant\", \"content\": gen_texts[i]}\n",
    "            ]\n",
    "            for i in range(len(gen_texts))\n",
    "        ]\n",
    "        chat_str = tokenizer.apply_chat_template(chats, tokenize=False)\n",
    "        enc = tokenizer(\n",
    "            chat_str,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            padding=True,\n",
    "            max_length=4096,\n",
    "        ).to(\"cuda\")\n",
    "        with torch.no_grad():\n",
    "            logits = rm_model(**enc).logits\n",
    "            p = F.softmax(logits, dim=-1)\n",
    "            ratings = torch.arange(1, 11, device=p.device).float()\n",
    "            exp_r = (p * ratings).sum(dim=1)  # Мат ожидание распределения оценок\n",
    "            ent = (-p * torch.log(p + 1e-8)).sum(dim=1)  # Энтропия\n",
    "            rewards = exp_r - entropy_coeff * ent\n",
    "\n",
    "        # REINFORCE loss с baseline\n",
    "        adv = mov_avg(rewards)\n",
    "        loss = -(adv * seq_logprob).mean()\n",
    "\n",
    "        # Шаг оптимизации\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Логирование\n",
    "        cum_loss += loss.item()\n",
    "        if n_iter and n_iter % 20 == 0:\n",
    "            print(f\"Iter {n_iter:05d}  avg_loss={cum_loss/20:.4f}\")\n",
    "            cum_loss = 0.0\n",
    "        if n_iter and n_iter % 200 == 0:\n",
    "            avg_r = eval_reward_dist(policy_model)\n",
    "            rewards_history.append(avg_r)\n",
    "            print(f\"\\n>> eval_reward: {avg_r:.4f}\\n\")\n",
    "\n",
    "        n_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "098f893b-36d5-45f9-9066-17d39c25f8a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.477266660005094,\n",
       " 3.7778431212294836,\n",
       " 4.118272331380972,\n",
       " 4.214707447440631,\n",
       " 4.2917454639005275,\n",
       " 4.329766364902977,\n",
       " 4.284382241021532,\n",
       " 4.200179849169529,\n",
       " 4.136497256583247,\n",
       " 4.180881199184755,\n",
       " 4.197088994545208,\n",
       " 4.063511586381027,\n",
       " 4.1756648793616815,\n",
       " 4.166365514811498,\n",
       " 4.242778195133158,\n",
       " 4.158104339170072,\n",
       " 3.991916802869086,\n",
       " 4.075044806457397]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rewards_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "befed789-8fe6-4101-973c-41190f578c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_model.save_pretrained('./policy_model_dist')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28377966-961d-4117-9c73-6d876f1efca7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Сравнение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbc6cd7-14d7-413b-8137-0ded1d697634",
   "metadata": {},
   "source": [
    "Так как модели обучены на абсолютно разных reward model, сравним reward на каждой из них"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4a22bd-fb2d-4dd5-9aef-4f06615a19eb",
   "metadata": {},
   "source": [
    "Сначала сравним на reward model из level 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38a6cbe8-67b8-404c-867f-1b2bd696080f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=576, out_features=10, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sft_model_name = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(sft_model_name)\n",
    "dist_policy_model = AutoModelForCausalLM.from_pretrained('./policy_model_dist').cuda()\n",
    "untrained_model = AutoModelForCausalLM.from_pretrained(sft_model_name).cuda()\n",
    "policy_model = AutoModelForCausalLM.from_pretrained('./policy_model_2').cuda()\n",
    "rm_model = AutoModelForSequenceClassification.from_pretrained('reward_model_with_dist/checkpoint-903').cuda()\n",
    "\n",
    "dist_policy_model.eval()\n",
    "untrained_model.eval()\n",
    "policy_model.eval()\n",
    "rm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9dea19-7d24-4282-ba31-5bd6a7b40c2b",
   "metadata": {},
   "source": [
    "Необученная"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e75ee7bd-c357-4908-9d90-cd10bd71a540",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fac83e30c964b2daef179de357501f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3.24191510964974"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_reward_dist(untrained_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd52bb8-cf7e-4792-a291-87a9111e5b06",
   "metadata": {},
   "source": [
    "Дообученная на level 2 rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6930d174-69bf-4759-83ed-74723b0d637b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e1334672134c8c980e7eb1fdc4431a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.032983042279772"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_reward_dist(dist_policy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600f6888-04bd-44a3-b8c8-1c98e9b72845",
   "metadata": {},
   "source": [
    "Дообученная на level 1 rm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e5fbece-e98c-4c6e-9e16-18ca0093b06e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e454bf3eb9234627813547bddf6decc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Eval:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "4.30431632637658"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_reward_dist(policy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805a05fd-934c-4fec-aa97-e899011ab155",
   "metadata": {},
   "source": [
    "Теперь сравним на reward model из level 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "76071ae1-4041-4e4c-ac46-7aaf4b142438",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForSequenceClassification(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(49152, 576, padding_idx=2)\n",
       "    (layers): ModuleList(\n",
       "      (0-29): 30 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "          (k_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (v_proj): Linear(in_features=576, out_features=192, bias=False)\n",
       "          (o_proj): Linear(in_features=576, out_features=576, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (up_proj): Linear(in_features=576, out_features=1536, bias=False)\n",
       "          (down_proj): Linear(in_features=1536, out_features=576, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((576,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=576, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rm_model = AutoModelForSequenceClassification.from_pretrained('reward_model_no_margin/checkpoint-903').cuda()\n",
    "rm_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb17e71a-55e1-411c-ac51-443a62f1199f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c92f33bd8c84665a523e56b95cb82ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-291.08098459243774"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_reward(untrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2baa2922-fa5e-4c70-828f-bb593cc7d594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "060a34ba8304449db05a4452e3709e00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-295.22210478782654"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_reward(dist_policy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3d4987f0-b66d-4b37-a9b9-b484f2761020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277414ecd86c466cb15c8213040d3a30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "-208.5806030035019"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_reward(policy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911672e1-dd42-4e58-bcf5-285448c40788",
   "metadata": {},
   "source": [
    "## Отчет по level 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bbe290-6e67-47cc-afb7-776274ba4d74",
   "metadata": {},
   "source": [
    "- Обучена reward модель, предсказывающая распределение над оценками (как именно см выше) с метрикой правильно проранжированных пар, близкой к reward model из level 1\n",
    "- Используя распределения из reward model level 2, дообучена sft модель (как именно см выше)\n",
    "- Однако средняя награда на reward model level 2 оказалась немного выше у sft модели level 1. Скорее всего, это связано с недообученной reward моделью, или плохо подобранным loss для дообучения sft level 2. Возможно, стоит брать другие характеристики распределения (мат ожидание и что-то еще помимо энтропии).\n",
    "- В награде на reward model level 1 также побеждает sft level 1 с большим отрывом. А sft level 2 показывает результат, близкий к недообученной модели. Я думаю, что причины те же, что и в предыдущем пункте. Здесь мог еще сыграть тот факт, что reward model level 2 сложнее, чем reward model level 1, а значит обучение у нее может быть более сложным/прихотливым. Обучали же мы их с заранее заданными параметрами"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b0240-a6be-4fd1-849c-92c4391fb2b6",
   "metadata": {},
   "source": [
    "В целом, выглядит, что эксперимент по level 2 неудачный. Дальнейшие шаги, которые стоит попробовать\n",
    "\n",
    "- Пробовать дообучить reward модели, чтобы они показывали лучшую метрику правильно проранжированных пар\n",
    "- Включить другие характеристики распределения в loss модели sft level 2\n",
    "- Пробовать разные гиперпараметры, в тч те, которые содержаться в loss'е и были введеные выше"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
